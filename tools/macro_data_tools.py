# tools/macro_data_tools.py â€” ê±°ì‹œê²½ì œ ë°ì´í„° + ë‰´ìŠ¤ ìˆ˜ì§‘ ë„êµ¬
# FRED API (VIX, DXY, TNX, SP500, USD/KRW)
# Google News RSS (í•œêµ­ ê²½ì œ/ì¦ì‹œ ë‰´ìŠ¤)
# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê²½ì œ í‚¤ì›Œë“œ)
# ê¸´ê¸‰ ë‰´ìŠ¤ ê°ì§€ (í‚¤ì›Œë“œ ê¸°ë°˜)

import os
import json
import time
import re
import xml.etree.ElementTree as ET
from datetime import datetime, date, timedelta
from urllib.parse import quote
from dotenv import load_dotenv

load_dotenv()

try:
    import requests
except ImportError:
    requests = None

try:
    import feedparser
except ImportError:
    feedparser = None

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FRED_API_KEY = os.getenv("FRED_API_KEY", "")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "")

# FRED ì‹œë¦¬ì¦ˆ ID
FRED_SERIES = {
    "VIX":    "VIXCLS",        # VIX ê³µí¬ì§€ìˆ˜
    "DXY":    "DTWEXBGS",      # ë‹¬ëŸ¬ ì¸ë±ìŠ¤ (Broad)
    "TNX":    "DGS10",         # ë¯¸êµ­ì±„ 10ë…„ ê¸ˆë¦¬
    "SP500":  "SP500",         # S&P 500
    "USDKRW": "DEXKOUS",      # ë‹¬ëŸ¬/ì› í™˜ìœ¨
    "FEDFUNDS": "FEDFUNDS",   # ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬
}

# ê¸´ê¸‰ ë‰´ìŠ¤ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨)
URGENT_KEYWORDS = {
    "ì„œí‚·ë¸Œë ˆì´ì»¤": 10, "ì‚¬ì´ë“œì¹´": 8,
    "ì „ìŸ": 9, "ë¯¸ì‚¬ì¼": 8, "í­ê²©": 9,
    "í­ë½": 7, "ê¸‰ë½": 6, "íŒ¨ë‹‰": 7,
    "ê¸ˆë¦¬ì¸ìƒ": 5, "ê¸´ê¸‰": 5,
    "ë””í´íŠ¸": 8, "íŒŒì‚°": 7, "ë¶€ë„": 7,
    "í…ŒëŸ¬": 9, "ê³„ì—„": 9,
    "ë¦¬ì„¸ì…˜": 6, "ê²½ê¸°ì¹¨ì²´": 6,
    "ë±…í¬ëŸ°": 8, "ìœ ë™ì„±ìœ„ê¸°": 7,
    "ë¸”ë™ë¨¼ë°ì´": 9, "ë¸”ë™ìŠ¤ì™„": 8,
}

_cache = {}
CACHE_TTL = 600  # 10ë¶„


# â”€â”€ 1. FRED API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_fred_series(series_id: str, days_back: int = 10) -> dict:
    """FREDì—ì„œ íŠ¹ì • ì‹œë¦¬ì¦ˆì˜ ìµœê·¼ ë°ì´í„° ì¡°íšŒ"""
    if not FRED_API_KEY or not requests:
        return {"value": 0.0, "date": "", "error": "FRED_API_KEY ë¯¸ì„¤ì •"}
    
    cache_key = f"fred_{series_id}"
    if cache_key in _cache and time.time() - _cache[cache_key]["ts"] < CACHE_TTL:
        return _cache[cache_key]["data"]
    
    end = date.today()
    start = end - timedelta(days=days_back)
    url = (
        f"https://api.stlouisfed.org/fred/series/observations"
        f"?series_id={series_id}"
        f"&api_key={FRED_API_KEY}"
        f"&file_type=json"
        f"&sort_order=desc"
        f"&limit=5"
        f"&observation_start={start.isoformat()}"
    )
    try:
        resp = requests.get(url, timeout=10)
        data = resp.json()
        obs = data.get("observations", [])
        # ìœ íš¨í•œ ê°’ ì°¾ê¸° (. ì€ ë¯¸ë°œí‘œ)
        for o in obs:
            if o["value"] != ".":
                result = {
                    "value": float(o["value"]),
                    "date": o["date"],
                }
                _cache[cache_key] = {"data": result, "ts": time.time()}
                return result
        return {"value": 0.0, "date": "", "error": "ë°ì´í„° ì—†ìŒ"}
    except Exception as e:
        return {"value": 0.0, "date": "", "error": str(e)}


def fetch_all_fred() -> dict:
    """ëª¨ë“  FRED ê±°ì‹œ ì§€í‘œë¥¼ í•œë²ˆì— ì¡°íšŒ"""
    results = {}
    for name, sid in FRED_SERIES.items():
        results[name] = fetch_fred_series(sid)
    return results


# â”€â”€ 2. yfinance ë³´ì™„ (ìµœê·¼ ì¢…ê°€ â€” ì£¼ë§ì—ë„ ê°€ëŠ¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_yfinance_recent() -> dict:
    """yfinanceë¡œ ìµœê·¼ 5ì¼ê°„ ì¢…ê°€ ì¡°íšŒ (ì£¼ë§ì—ë„ ì§ì „ ê±°ë˜ì¼ ë°ì´í„° ë°˜í™˜)"""
    try:
        import yfinance as yf
    except ImportError:
        return {}
    
    symbols = {
        "VIX": "^VIX", "DXY": "DX-Y.NYB", "TNX": "^TNX",
        "SP500": "^GSPC", "USDKRW": "USDKRW=X", "KOSPI": "^KS11",
    }
    results = {}
    for name, ticker in symbols.items():
        try:
            df = yf.download(ticker, period="5d", progress=False)
            if not df.empty:
                last = df.iloc[-1]
                prev = df.iloc[-2] if len(df) > 1 else last
                close_val = float(last["Close"].iloc[0]) if hasattr(last["Close"], "iloc") else float(last["Close"])
                prev_val = float(prev["Close"].iloc[0]) if hasattr(prev["Close"], "iloc") else float(prev["Close"])
                chg = ((close_val - prev_val) / prev_val * 100) if prev_val else 0
                results[name] = {
                    "value": round(close_val, 2),
                    "change_pct": round(chg, 2),
                    "date": str(df.index[-1].date()),
                }
            else:
                results[name] = {"value": 0.0, "change_pct": 0.0}
        except Exception:
            results[name] = {"value": 0.0, "change_pct": 0.0}
    return results


# â”€â”€ 3. ë‰´ìŠ¤ ìˆ˜ì§‘ (Google News RSS + ë„¤ì´ë²„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_google_news_rss(query: str = "í•œêµ­ ê²½ì œ ì¦ì‹œ", max_items: int = 15) -> list:
    """Google News RSSë¡œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘"""
    if feedparser is None:
        return _fetch_google_news_fallback(query, max_items)
    
    encoded = quote(query)
    url = f"https://news.google.com/rss/search?q={encoded}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:max_items]:
            articles.append({
                "title": entry.get("title", ""),
                "source": entry.get("source", {}).get("title", "") if hasattr(entry.get("source", {}), "get") else "",
                "published": entry.get("published", ""),
                "link": entry.get("link", ""),
            })
        return articles
    except Exception:
        return _fetch_google_news_fallback(query, max_items)


def _fetch_google_news_fallback(query: str, max_items: int) -> list:
    """feedparser ì—†ì„ ë•Œ requests + XML íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´"""
    if not requests:
        return []
    encoded = quote(query)
    url = f"https://news.google.com/rss/search?q={encoded}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    try:
        resp = requests.get(url, timeout=10)
        root = ET.fromstring(resp.content)
        articles = []
        for item in root.findall(".//item")[:max_items]:
            articles.append({
                "title": item.findtext("title", ""),
                "source": item.findtext("source", ""),
                "published": item.findtext("pubDate", ""),
                "link": item.findtext("link", ""),
            })
        return articles
    except Exception:
        return []


def fetch_naver_news_search(query: str = "ì¦ì‹œ ì „ë§", max_items: int = 10) -> list:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    if not requests or not NAVER_CLIENT_ID:
        return []
    
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": max_items, "sort": "date"}
    
    try:
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        data = resp.json()
        articles = []
        for item in data.get("items", []):
            title = re.sub(r"<[^>]+>", "", item.get("title", ""))
            desc = re.sub(r"<[^>]+>", "", item.get("description", ""))
            articles.append({
                "title": title,
                "description": desc[:100],
                "published": item.get("pubDate", ""),
                "link": item.get("link", ""),
            })
        return articles
    except Exception:
        return []


def collect_macro_news(max_total: int = 20) -> list:
    """ê²½ì œ/ì¦ì‹œ ë‰´ìŠ¤ë¥¼ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì¢…í•© ìˆ˜ì§‘"""
    all_news = []
    
    # Google News RSS â€” ì—¬ëŸ¬ í‚¤ì›Œë“œ
    for q in ["í•œêµ­ ì¦ì‹œ", "ì½”ìŠ¤í”¼ ì „ë§", "ë¯¸êµ­ ê²½ì œ ê¸ˆë¦¬"]:
        articles = fetch_google_news_rss(q, max_items=8)
        all_news.extend(articles)
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ (API í‚¤ ìˆì„ ë•Œ)
    for q in ["ì¦ì‹œ ì „ë§", "ê²½ì œ ê¸ˆë¦¬"]:
        articles = fetch_naver_news_search(q, max_items=5)
        all_news.extend(articles)
    
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    seen = set()
    unique = []
    for a in all_news:
        title = a.get("title", "").strip()
        if title and title not in seen:
            seen.add(title)
            unique.append(a)
    
    return unique[:max_total]


# â”€â”€ 4. ê¸´ê¸‰ ë‰´ìŠ¤ ê°ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_urgent_news(news_list: list = None) -> dict:
    """ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ê¸´ê¸‰ í‚¤ì›Œë“œë¥¼ íƒì§€í•˜ê³  ìœ„í—˜ë„ ì ìˆ˜ ë°˜í™˜"""
    if news_list is None:
        news_list = collect_macro_news(max_total=30)
    
    urgent_items = []
    total_score = 0
    
    for article in news_list:
        title = article.get("title", "")
        desc = article.get("description", "")
        text = f"{title} {desc}"
        
        matched = {}
        for keyword, weight in URGENT_KEYWORDS.items():
            if keyword in text:
                matched[keyword] = weight
        
        if matched:
            score = sum(matched.values())
            total_score += score
            urgent_items.append({
                "title": title,
                "keywords": list(matched.keys()),
                "score": score,
            })
    
    # ìœ„í—˜ ë“±ê¸‰ íŒì •
    if total_score >= 20:
        level = "CRITICAL"   # ì¦‰ì‹œ ì „ëŸ‰ ë§¤ë„ ê¶Œê³ 
    elif total_score >= 10:
        level = "HIGH"       # ì‹ ê·œ ë§¤ìˆ˜ ì¤‘ë‹¨ + ë¶€ë¶„ ì²­ì‚°
    elif total_score >= 5:
        level = "MEDIUM"     # í¬ì§€ì…˜ ì¶•ì†Œ ê¶Œê³ 
    else:
        level = "LOW"        # ì •ìƒ
    
    return {
        "level": level,
        "total_score": total_score,
        "urgent_count": len(urgent_items),
        "urgent_items": urgent_items[:5],  # ìƒìœ„ 5ê°œë§Œ
        "checked_at": datetime.now().isoformat(),
    }


# â”€â”€ 5. ì¢…í•© ê±°ì‹œê²½ì œ ë°ì´í„° ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def collect_all_macro_data() -> dict:
    """FRED + yfinance + ë‰´ìŠ¤ë¥¼ ëª¨ë‘ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ dictë¡œ ë°˜í™˜"""
    result = {
        "timestamp": datetime.now().isoformat(),
    }
    
    # 1) FRED ë°ì´í„°
    print("  ğŸ“Š FRED ê±°ì‹œì§€í‘œ ìˆ˜ì§‘ ì¤‘...")
    fred_data = fetch_all_fred()
    result["fred"] = fred_data
    
    # 2) yfinance ìµœê·¼ ì¢…ê°€
    print("  ğŸ“ˆ yfinance ìµœê·¼ ì¢…ê°€ ìˆ˜ì§‘ ì¤‘...")
    yf_data = fetch_yfinance_recent()
    result["yfinance"] = yf_data
    
    # 3) ê±°ì‹œ ë°ì´í„° í†µí•© (FRED ìš°ì„ , yfinance ë³´ì™„)
    merged = {}
    for key in ["VIX", "DXY", "TNX", "SP500", "USDKRW"]:
        fred_val = fred_data.get(key, {})
        yf_val = yf_data.get(key, {})
        
        if fred_val.get("value", 0) > 0:
            merged[key] = {
                "value": fred_val["value"],
                "date": fred_val.get("date", ""),
                "source": "FRED",
            }
        elif yf_val.get("value", 0) > 0:
            merged[key] = {
                "value": yf_val["value"],
                "change_pct": yf_val.get("change_pct", 0),
                "date": yf_val.get("date", ""),
                "source": "yfinance",
            }
        else:
            merged[key] = {"value": 0.0, "source": "ì—†ìŒ"}
    
    # KOSPIëŠ” yfinanceë§Œ
    kospi = yf_data.get("KOSPI", {})
    merged["KOSPI"] = {
        "value": kospi.get("value", 0),
        "change_pct": kospi.get("change_pct", 0),
        "source": "yfinance",
    }
    
    # FEDFUNDSëŠ” FREDë§Œ
    ff = fred_data.get("FEDFUNDS", {})
    merged["FEDFUNDS"] = {
        "value": ff.get("value", 0),
        "date": ff.get("date", ""),
        "source": "FRED",
    }
    
    result["macro_data"] = merged
    
    # 4) ë‰´ìŠ¤ ìˆ˜ì§‘
    print("  ğŸ“° ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    news = collect_macro_news(max_total=20)
    result["news"] = news
    result["news_count"] = len(news)
    
    # 5) ê¸´ê¸‰ ë‰´ìŠ¤ ì²´í¬
    urgent = check_urgent_news(news)
    result["urgent"] = urgent
    
    print(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì§€í‘œ {len(merged)}ê°œ, ë‰´ìŠ¤ {len(news)}ê±´, ê¸´ê¸‰={urgent['level']}")
    
    return result


# â”€â”€ í…ŒìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("=" * 60)
    print(" MacroDataTools í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    data = collect_all_macro_data()
    
    print(f"\nê±°ì‹œ ì§€í‘œ:")
    for k, v in data["macro_data"].items():
        print(f"  {k}: {v}")
    
    print(f"\në‰´ìŠ¤ {data['news_count']}ê±´:")
    for n in data["news"][:5]:
        print(f"  - {n['title'][:60]}")
    
    print(f"\nê¸´ê¸‰ ë‰´ìŠ¤ ë ˆë²¨: {data['urgent']['level']} (ì ìˆ˜: {data['urgent']['total_score']})")
