"""
run_extended.py â€” í™•ì¥ ë°±í…ŒìŠ¤íŠ¸ (numpy ë²¡í„°í™” + 2ë‹¨ê³„ ìµœì í™”)
=============================================================================
Stage 1: numpy ë°°ì—´ ê¸°ë°˜ ì´ˆê³ ì† í”„ë¡ì‹œ í‰ê°€ (50,000íšŒ, ~1ë¶„)
Stage 2: ìƒìœ„ 500ê°œ ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜ (~500 Ã— 0.6s = 5ë¶„)
Walk-Forward: 3-fold Ã— 2,000 Stage1 + 100 Stage2
ì´ ëª©í‘œ: 25ë¶„ ì´ë‚´
"""
import os, sys, time, random, logging
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
from dataclasses import dataclass
from scipy import stats

sys.path.insert(0, str(Path(__file__).parent.parent))

OUT_DIR = Path("analysis/results/extended")
OUT_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(
            f"analysis/results/extended/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        ),
    ],
)
logger = logging.getLogger("run_extended")

FULL_START  = "20230901"
FULL_END    = "20260224"
TRAIN_START = "20230901"
TRAIN_END   = "20241231"
TEST_START  = "20250101"
TEST_END    = "20260224"

WF_FOLDS = [
    {"train_start": "20230901", "train_end": "20240630",
     "test_start":  "20240701", "test_end":  "20241231"},
    {"train_start": "20230901", "train_end": "20241231",
     "test_start":  "20250101", "test_end":  "20250630"},
    {"train_start": "20230901", "train_end": "20250630",
     "test_start":  "20250701", "test_end":  "20260224"},
]

GRID = {
    "dc_period":       [10, 15, 20, 25, 30, 40],
    "vol_ratio_min":   [1.5, 2.0, 2.5, 3.0],
    "adx_min":         [15.0, 20.0, 25.0, 30.0, 35.0],
    "rsi_min":         [30.0, 35.0, 40.0, 45.0, 50.0],
    "rsi_max":         [65.0, 70.0, 75.0, 80.0, 85.0],
    "atr_stop_mult":   [1.0, 1.5, 2.0, 2.5, 3.0],
    "trail_stop_pct":  [0.02, 0.03, 0.04, 0.05, 0.07],
    "take_profit":     [0.07, 0.08, 0.10, 0.12, 0.15, 0.20],
    "time_stop_days":  [3, 5, 7, 10, 15, 20],
}


@dataclass
class P:
    dc_period:       int   = 20
    vol_ratio_min:   float = 2.0
    adx_min:         float = 25.0
    rsi_min:         float = 40.0
    rsi_max:         float = 75.0
    atr_stop_mult:   float = 2.0
    trail_stop_pct:  float = 0.03
    take_profit:     float = 0.10
    time_stop_days:  int   = 7
    max_positions:   int   = 5
    position_size:   float = 0.20

    @staticmethod
    def sample():
        return P(
            dc_period      = random.choice(GRID["dc_period"]),
            vol_ratio_min  = random.choice(GRID["vol_ratio_min"]),
            adx_min        = random.choice(GRID["adx_min"]),
            rsi_min        = random.choice(GRID["rsi_min"]),
            rsi_max        = random.choice(GRID["rsi_max"]),
            atr_stop_mult  = random.choice(GRID["atr_stop_mult"]),
            trail_stop_pct = random.choice(GRID["trail_stop_pct"]),
            take_profit    = random.choice(GRID["take_profit"]),
            time_stop_days = random.choice(GRID["time_stop_days"]),
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# numpy ë²¡í„° ìºì‹œ (Stage1 ì´ˆê³ ì† í‰ê°€ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class NumpyCache:
    """
    DC ê¸°ê°„ë³„ë¡œ í•„ìš”í•œ ë°°ì—´ì„ numpy í˜•íƒœë¡œ ë¯¸ë¦¬ ì¶”ì¶œ.
    Stage1 í•„í„°ë§ì„ pandas ì—†ì´ ìˆœìˆ˜ numpyë¡œ ìˆ˜í–‰ â†’ 100ë°° ë¹ ë¦„.
    """
    def __init__(self, df: pd.DataFrame):
        self.dc_caches: Dict[int, Dict[str, np.ndarray]] = {}
        self._build(df)

    def _build(self, df: pd.DataFrame):
        logger.info("  numpy ë²¡í„° ìºì‹œ ë¹Œë“œ ì¤‘...")
        for dc in GRID["dc_period"]:
            tmp = df.copy()
            dc_col = f"dc_high{dc}"
            if dc_col not in tmp.columns:
                tmp[dc_col] = tmp.groupby("ticker")["high"].transform(
                    lambda x: x.shift(1).rolling(dc).max()
                )
            # ë‚ ì§œ ë²”ìœ„ ë§ˆìŠ¤í¬ ë¯¸ë¦¬ ê³„ì‚°
            dates = tmp["date"].values

            # ë‚ ì§œë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (ë¬¸ìì—´ ë¹„êµë³´ë‹¤ 10ë°° ë¹ ë¦„)
            date_int = dates.astype(np.int32) if hasattr(dates, 'astype') else \
                       np.array([int(d) for d in dates], dtype=np.int32)

            self.dc_caches[dc] = {
                "date":       dates,
                "date_int":   date_int,   # ì •ìˆ˜í˜• ë‚ ì§œ (ë¹ ë¥¸ ë²”ìœ„ ë¹„êµìš©)
                "close":      tmp["close"].values.astype(np.float32),
                "ma60":       tmp["ma60"].values.astype(np.float32),
                "vol_ratio":  tmp["vol_ratio"].values.astype(np.float32),
                "adx14":      tmp["adx14"].values.astype(np.float32),
                "rsi14":      tmp["rsi14"].values.astype(np.float32),
                "atr14":      tmp["atr14"].values.astype(np.float32),
                "sig_dc":     (tmp["close"].values > tmp[dc_col].values).astype(np.bool_),
                "sig_ma":     (tmp["close"].values > tmp["ma60"].values).astype(np.bool_),
                "fwd_ret5":   tmp["fwd_ret5"].values.astype(np.float32),
                "ticker":     tmp["ticker"].values,
                "_df":        tmp,   # Stage2ìš© ì „ì²´ DataFrame
            }
        logger.info(f"  numpy ìºì‹œ ì™„ë£Œ: {len(self.dc_caches)}ê°œ DC ê¸°ê°„")


def numpy_proxy_score(nc: NumpyCache, p: P, start: str, end: str) -> float:
    """
    numpy ë°°ì—´ë§Œ ì‚¬ìš©í•œ ì´ˆê³ ì† í”„ë¡ì‹œ ì ìˆ˜.
    DataFrame ë³µì‚¬ ì—†ì´ boolean maskingë§Œ ìˆ˜í–‰.
    """
    c = nc.dc_caches[p.dc_period]

    # ë‚ ì§œ ë²”ìœ„ ë§ˆìŠ¤í¬ (ì •ìˆ˜ ë¹„êµë¡œ ê°€ì†)
    s_int = int(start)
    e_int = int(end)
    date_mask = (c["date_int"] >= s_int) & (c["date_int"] <= e_int)

    # ì‹ í˜¸ í•„í„° ë§ˆìŠ¤í¬
    mask = (
        date_mask &
        c["sig_dc"] &
        c["sig_ma"] &
        (c["vol_ratio"] >= p.vol_ratio_min) &
        (c["adx14"]     >= p.adx_min) &
        (c["rsi14"]     >= p.rsi_min) &
        (c["rsi14"]     <= p.rsi_max)
    )

    rets = c["fwd_ret5"][mask]
    # NaN ì œê±°
    rets = rets[~np.isnan(rets)]
    n = len(rets)
    if n < 20:
        return 0.0

    mean_r = float(np.mean(rets))
    std_r  = float(np.std(rets))
    if std_r < 1e-9:
        return 0.0

    proxy = mean_r / std_r * np.sqrt(252)
    # ê±°ë˜ëŸ‰ ë³´ì • (ë§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ)
    vol_bonus = np.sqrt(min(n, 500) / 100)
    return float(proxy * vol_bonus)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Stage2: ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_full_backtest(nc: NumpyCache, p: P,
                       start: str, end: str,
                       capital: float = 100_000_000) -> Dict:
    """í¬íŠ¸í´ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜ (NumpyCacheì˜ _df ì‚¬ìš©)"""
    df = nc.dc_caches[p.dc_period]["_df"]

    # ì‹ í˜¸ ì ìš©
    dc = p.dc_period
    dc_col = f"dc_high{dc}"

    tmp = df.copy()
    tmp["sig_dc"]  = tmp["close"] > tmp[dc_col]
    tmp["sig_vol"] = tmp["vol_ratio"] >= p.vol_ratio_min
    tmp["sig_adx"] = tmp["adx14"] >= p.adx_min
    tmp["sig_rsi"] = (tmp["rsi14"] >= p.rsi_min) & (tmp["rsi14"] <= p.rsi_max)
    tmp["sig_ma"]  = tmp["close"] > tmp["ma60"]
    tmp["entry_signal"] = (
        tmp["sig_dc"] & tmp["sig_vol"] & tmp["sig_adx"] &
        tmp["sig_rsi"] & tmp["sig_ma"]
    )

    # weekday/month ì¶”ê°€
    try:
        tmp["weekday"] = pd.to_datetime(tmp["date"], format="%Y%m%d").dt.dayofweek
        tmp["month"]   = pd.to_datetime(tmp["date"], format="%Y%m%d").dt.month
    except Exception:
        tmp["weekday"] = -1
        tmp["month"]   = -1

    data = tmp[(tmp["date"] >= start) & (tmp["date"] <= end)]
    if data.empty:
        return _empty_result()

    needed = ["close", "atr14", "vol_ratio", "entry_signal", "weekday", "month"]
    needed = [c for c in needed if c in data.columns]
    data_dict = {}
    for dt, grp in data.groupby("date"):
        data_dict[dt] = grp.set_index("ticker")[needed].to_dict("index")

    dates = sorted(data_dict.keys())
    positions = {}
    cash = capital
    equity_curve = []
    trades = []

    for date in dates:
        day = data_dict.get(date, {})

        # ì²­ì‚° ì²´í¬
        to_close = []
        for ticker, pos in positions.items():
            row = day.get(ticker)
            pos["hold_days"] += 1
            if row is None:
                continue
            price = float(row["close"])
            pos["peak"] = max(pos["peak"], price)
            trail_stop = pos["peak"] * (1 - p.trail_stop_pct)
            eff_stop   = max(pos["stop"], trail_stop)

            exit_r = None
            exit_p = price
            if price <= eff_stop:
                exit_r = "stop"
                exit_p = max(eff_stop, price * 0.98)
            elif (price / pos["entry_price"] - 1) >= p.take_profit:
                exit_r = "take_profit"
            elif pos["hold_days"] >= p.time_stop_days:
                exit_r = "time_stop"

            if exit_r:
                ret = exit_p / pos["entry_price"] - 1
                ev  = pos["alloc"] * (1 + ret)
                cash += ev
                trades.append({
                    "ticker":     ticker,
                    "entry_date": pos["entry_date"],
                    "exit_date":  date,
                    "entry_price": pos["entry_price"],
                    "exit_price": exit_p,
                    "ret":        ret,
                    "hold_days":  pos["hold_days"],
                    "exit_reason": exit_r,
                    "weekday":    pos.get("weekday", -1),
                    "month":      pos.get("month", -1),
                })
                to_close.append(ticker)

        for t in to_close:
            del positions[t]

        # ì§„ì…
        if len(positions) < p.max_positions:
            sigs = [(t, float(r.get("vol_ratio", 0)))
                    for t, r in day.items()
                    if r.get("entry_signal") == True and t not in positions]
            sigs.sort(key=lambda x: x[1], reverse=True)

            slots = p.max_positions - len(positions)
            for ticker, _ in sigs[:slots]:
                row   = day[ticker]
                price = float(row["close"])
                pos_val = sum(
                    pos["alloc"] * (float(day[t]["close"]) / pos["entry_price"]
                                    if t in day else 1.0)
                    for t, pos in positions.items()
                )
                alloc = (cash + pos_val) * p.position_size
                if alloc > cash:
                    continue
                atr  = float(row.get("atr14", price * 0.02))
                stop = price - p.atr_stop_mult * atr
                cash -= alloc
                positions[ticker] = {
                    "entry_date":  date,
                    "entry_price": price,
                    "stop":        stop,
                    "peak":        price,
                    "alloc":       alloc,
                    "hold_days":   0,
                    "weekday":     int(row.get("weekday", -1)),
                    "month":       int(row.get("month", -1)),
                }

        pos_val = sum(
            pos["alloc"] * (float(day[t]["close"]) / pos["entry_price"]
                            if t in day else 1.0)
            for t, pos in positions.items()
        )
        equity_curve.append({"date": date, "equity": cash + pos_val})

    if len(trades) < 10:
        return {**_empty_result(), "trades": trades,
                "equity_curve": equity_curve, "trade_count": len(trades)}

    eq = pd.DataFrame(equity_curve).set_index("date")["equity"]
    dr = eq.pct_change().dropna()
    rets = [t["ret"] for t in trades]
    wins = [r for r in rets if r > 0]
    loss = [r for r in rets if r <= 0]

    return {
        "sharpe":        float(dr.mean() / (dr.std() + 1e-9) * np.sqrt(252)),
        "total_return":  float(eq.iloc[-1] / eq.iloc[0] - 1),
        "mdd":           float(((eq - eq.cummax()) / eq.cummax()).min()),
        "win_rate":      len(wins) / len(rets),
        "trade_count":   len(rets),
        "avg_hold_days": float(np.mean([t["hold_days"] for t in trades])),
        "profit_factor": sum(wins) / (abs(sum(loss)) + 1e-9),
        "trades":        trades,
        "equity_curve":  equity_curve,
        "params":        p,
    }


def _empty_result():
    return {"sharpe": 0, "total_return": 0, "mdd": 0, "win_rate": 0,
            "trade_count": 0, "avg_hold_days": 0, "profit_factor": 0,
            "trades": [], "equity_curve": []}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ë‹¨ê³„ ìµœì í™”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def two_stage_optimize(nc: NumpyCache,
                        n_stage1: int = 50000,
                        n_stage2: int = 500,
                        top_k: int = 50) -> Tuple[List[Dict], pd.DataFrame]:
    logger.info(f"\n[2ë‹¨ê³„ ìµœì í™”] Stage1: {n_stage1:,}íšŒ â†’ Stage2: {n_stage2}íšŒ")

    # â”€â”€ Stage 1: ì´ˆê³ ì† í”„ë¡ì‹œ ì ìˆ˜ â”€â”€
    logger.info(f"\n  [Stage 1] {n_stage1:,}íšŒ numpy ë²¡í„° í‰ê°€...")
    t0 = time.time()

    s1 = []
    for i in range(n_stage1):
        if i % 5000 == 0 and i > 0:
            elapsed = time.time() - t0
            eta = elapsed / i * (n_stage1 - i)
            logger.info(f"    {i:,}/{n_stage1:,}... ETA={eta:.0f}s "
                        f"(ìœ íš¨:{len(s1)}ê°œ)")
        p = P.sample()
        score = numpy_proxy_score(nc, p, TRAIN_START, TRAIN_END)
        if score > 0:
            s1.append((score, p))

    s1.sort(key=lambda x: x[0], reverse=True)
    logger.info(f"  Stage 1 ì™„ë£Œ: {len(s1)}ê°œ ìœ íš¨, ì†Œìš”={time.time()-t0:.1f}ì´ˆ")

    if not s1:
        return [], pd.DataFrame()

    # â”€â”€ Stage 2: ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ì‹œë®¬ â”€â”€
    candidates = [p for _, p in s1[:n_stage2]]
    logger.info(f"\n  [Stage 2] {len(candidates)}ê°œ ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ì‹œë®¬...")
    t0 = time.time()

    s2 = []
    for i, p in enumerate(candidates):
        if i % 50 == 0 and i > 0:
            elapsed = time.time() - t0
            eta = elapsed / i * (len(candidates) - i)
            logger.info(f"    {i}/{len(candidates)}... ETA={eta:.0f}s "
                        f"(ìœ íš¨:{len(s2)}ê°œ)")
        r = run_full_backtest(nc, p, TRAIN_START, TRAIN_END)
        if r["trade_count"] >= 20:
            s2.append(r)

    s2.sort(key=lambda x: x["sharpe"], reverse=True)
    logger.info(f"  Stage 2 ì™„ë£Œ: {len(s2)}ê°œ ìœ íš¨, ì†Œìš”={time.time()-t0:.1f}ì´ˆ")

    # ì €ì¥
    rows = []
    for r in s2[:top_k]:
        p = r["params"]
        rows.append({
            "dc_period":      p.dc_period,
            "vol_ratio_min":  p.vol_ratio_min,
            "adx_min":        p.adx_min,
            "rsi_min":        p.rsi_min,
            "rsi_max":        p.rsi_max,
            "atr_stop_mult":  p.atr_stop_mult,
            "trail_stop_pct": p.trail_stop_pct,
            "take_profit":    p.take_profit,
            "time_stop_days": p.time_stop_days,
            "total_return%":  round(r["total_return"] * 100, 2),
            "sharpe":         round(r["sharpe"], 3),
            "mdd%":           round(r["mdd"] * 100, 2),
            "win_rate%":      round(r["win_rate"] * 100, 1),
            "trade_count":    r["trade_count"],
            "avg_hold_days":  round(r["avg_hold_days"], 1),
            "profit_factor":  round(r["profit_factor"], 3),
        })

    df_out = pd.DataFrame(rows)
    df_out.to_csv(OUT_DIR / "ext_top_params.csv", index=False)
    _param_distribution(df_out)
    return s2[:top_k], df_out


def _param_distribution(df: pd.DataFrame):
    if df.empty:
        return
    logger.info("\n  [íŒŒë¼ë¯¸í„° ë¶„í¬] ìƒìœ„ ê²°ê³¼ ìˆ˜ë ´ë„:")
    cols = ["dc_period", "vol_ratio_min", "adx_min",
            "trail_stop_pct", "take_profit", "time_stop_days"]
    rows = []
    for col in [c for c in cols if c in df.columns]:
        vc = df[col].value_counts(normalize=True)
        v, pct = vc.index[0], vc.iloc[0] * 100
        rows.append({"íŒŒë¼ë¯¸í„°": col, "ìµœë¹ˆê°’": v, "ì§‘ì¤‘ë„%": round(pct, 1)})
        logger.info(f"    {col}: ìµœë¹ˆê°’={v} ({pct:.0f}%)")
    pd.DataFrame(rows).to_csv(OUT_DIR / "param_distribution.csv", index=False)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì›Œí¬í¬ì›Œë“œ ê²€ì¦ (3-fold)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_walk_forward(nc: NumpyCache) -> pd.DataFrame:
    logger.info("\n[Walk-Forward] 3-fold ê²€ì¦...")
    results = []

    for fi, fold in enumerate(WF_FOLDS):
        logger.info(f"\n  Fold {fi+1}/3: "
                    f"í•™ìŠµ={fold['train_start']}~{fold['train_end']}, "
                    f"ê²€ì¦={fold['test_start']}~{fold['test_end']}")

        # ì´ fold í•™ìŠµê¸°ê°„ì—ì„œ 3,000 Stage1 â†’ ìƒìœ„ 80 Stage2
        s1 = []
        for _ in range(3000):
            p = P.sample()
            s = numpy_proxy_score(nc, p, fold["train_start"], fold["train_end"])
            if s > 0:
                s1.append((s, p))

        s1.sort(key=lambda x: x[0], reverse=True)

        fold_best = None
        fold_best_sharpe = 0.0
        for _, p in s1[:80]:
            r = run_full_backtest(nc, p, fold["train_start"], fold["train_end"])
            if r["trade_count"] >= 15 and r["sharpe"] > fold_best_sharpe:
                fold_best_sharpe = r["sharpe"]
                fold_best = p

        if fold_best is None:
            logger.warning(f"  Fold {fi+1}: ìœ íš¨ ê²°ê³¼ ì—†ìŒ")
            continue

        # ê²€ì¦
        test_r = run_full_backtest(nc, fold_best, fold["test_start"], fold["test_end"])
        ovfit  = test_r["sharpe"] / (fold_best_sharpe + 1e-9)

        results.append({
            "fold":             fi + 1,
            "train_start":      fold["train_start"],
            "train_end":        fold["train_end"],
            "test_start":       fold["test_start"],
            "test_end":         fold["test_end"],
            "train_sharpe":     round(fold_best_sharpe, 3),
            "test_sharpe":      round(test_r["sharpe"], 3),
            "test_return%":     round(test_r["total_return"] * 100, 2),
            "test_mdd%":        round(test_r["mdd"] * 100, 2),
            "test_win_rate%":   round(test_r["win_rate"] * 100, 1),
            "test_trades":      test_r["trade_count"],
            "overfitting_ratio": round(ovfit, 3),
            "dc_period":        fold_best.dc_period,
            "trail_stop_pct":   fold_best.trail_stop_pct,
            "take_profit":      fold_best.take_profit,
        })
        logger.info(f"  Fold {fi+1}: í•™ìŠµ={fold_best_sharpe:.2f}, "
                    f"ê²€ì¦={test_r['sharpe']:.2f}, "
                    f"ìˆ˜ìµ={test_r['total_return']*100:.1f}%, "
                    f"ê³¼ì í•©={ovfit:.2f}")

    df_out = pd.DataFrame(results)
    df_out.to_csv(OUT_DIR / "walk_forward.csv", index=False)
    if not df_out.empty:
        logger.info(f"\n  í‰ê·  ê³¼ì í•©ë¹„ìœ¨: {df_out['overfitting_ratio'].mean():.2f}")
    return df_out


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ëª¬í…Œì¹´ë¥¼ë¡œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_monte_carlo(result: Dict, n_sim: int = 2000) -> Dict:
    logger.info(f"\n[ëª¬í…Œì¹´ë¥¼ë¡œ] {n_sim:,}íšŒ...")
    trades = result.get("trades", [])
    if len(trades) < 20:
        logger.warning(f"ê±°ë˜ ìˆ˜ ë¶€ì¡± ({len(trades)})")
        return {}

    rets    = np.array([t["ret"] for t in trades], dtype=np.float32)
    max_pos = result["params"].max_positions
    ps      = result["params"].position_size

    mc_s, mc_r, mc_m = [], [], []
    for _ in range(n_sim):
        idx     = np.random.permutation(len(rets))
        shuffled = rets[idx]

        eq = [1.0]
        cap = 1.0
        for i in range(0, len(shuffled), max_pos):
            batch = shuffled[i:i+max_pos]
            cap  *= (1 + float(np.mean(batch)) * ps * len(batch))
            eq.append(cap)

        eq_s = pd.Series(eq)
        dr   = eq_s.pct_change().dropna()
        mc_s.append(float(dr.mean() / (dr.std() + 1e-9) * np.sqrt(252 / max_pos)))
        mc_r.append(float(eq_s.iloc[-1] - 1))
        mc_m.append(float(((eq_s - eq_s.cummax()) / eq_s.cummax()).min()))

    mc_s = np.array(mc_s)
    mc_r = np.array(mc_r)
    mc_m = np.array(mc_m)

    res = {
        "n_sim":           n_sim,
        "n_trades":        len(rets),
        "actual_sharpe":   round(result["sharpe"], 3),
        "actual_return%":  round(result["total_return"] * 100, 2),
        "actual_mdd%":     round(result["mdd"] * 100, 2),
        "sharpe_mean":     round(float(mc_s.mean()), 3),
        "sharpe_std":      round(float(mc_s.std()), 3),
        "sharpe_p5":       round(float(np.percentile(mc_s, 5)), 3),
        "sharpe_p95":      round(float(np.percentile(mc_s, 95)), 3),
        "sharpe_pos_prob%": round(float((mc_s > 0).mean() * 100), 1),
        "return_mean%":    round(float(mc_r.mean() * 100), 2),
        "return_p5%":      round(float(np.percentile(mc_r, 5) * 100), 2),
        "return_p95%":     round(float(np.percentile(mc_r, 95) * 100), 2),
        "mdd_mean%":       round(float(mc_m.mean() * 100), 2),
        "mdd_worst5%":     round(float(np.percentile(mc_m, 95) * 100), 2),
    }
    pd.DataFrame([res]).to_csv(OUT_DIR / "monte_carlo.csv", index=False)
    logger.info(f"  Sharpe: {res['sharpe_mean']} Â±{res['sharpe_std']}, "
                f"ì–‘ì˜í™•ë¥ ={res['sharpe_pos_prob%']}%")
    logger.info(f"  ìˆ˜ìµë¥  95CI: [{res['return_p5%']}%, {res['return_p95%']}%]")
    logger.info(f"  MDD ìµœì•…5%: {res['mdd_worst5%']}%")
    return res


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¹í„°ë³„ ì„±ê³¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SECTORS = {
    "ë°˜ë„ì²´":    ["005930", "000660", "091160", "229200", "042700"],
    "2ì°¨ì „ì§€":   ["006400", "373220", "051910", "000270", "096770"],
    "ë°”ì´ì˜¤/ì œì•½": ["207940", "068270", "326030", "086900", "145020"],
    "ê¸ˆìœµ":      ["105560", "055550", "086790", "316140", "032830"],
    "ìë™ì°¨":    ["005380", "012330", "000270", "011210", "003620"],
    "ì¡°ì„ /ë°©ì‚°": ["010140", "042660", "006360", "047050", "272210"],
    "ì—”í„°/ë¯¸ë””ì–´": ["041510", "035900", "352820", "122870"],
}

def analyze_sectors(nc: NumpyCache, daily_df: pd.DataFrame, best_p: P) -> pd.DataFrame:
    logger.info("\n[ì„¹í„° ë¶„ì„]...")
    rows = []

    # ì „ì²´ ë² ì´ìŠ¤ë¼ì¸
    base = run_full_backtest(nc, best_p, FULL_START, FULL_END)
    rows.append({
        "sector": "ì „ì²´(ë² ì´ìŠ¤)", "n_tickers": daily_df["ticker"].nunique(),
        "total_return%": round(base["total_return"]*100, 2),
        "sharpe": round(base["sharpe"], 3), "mdd%": round(base["mdd"]*100, 2),
        "win_rate%": round(base["win_rate"]*100, 1), "trade_count": base["trade_count"],
        "avg_hold_days": round(base["avg_hold_days"], 1),
    })
    logger.info(f"  ì „ì²´: ìˆ˜ìµ={base['total_return']*100:.1f}%, "
                f"Sharpe={base['sharpe']:.3f}, ê±°ë˜={base['trade_count']}")

    all_tickers = set(daily_df["ticker"].unique())
    for sector, tickers in SECTORS.items():
        sec_tickers = set(t for t in tickers if t in all_tickers)
        if not sec_tickers:
            continue

        # ì„¹í„° ì¢…ëª©ë§Œ í•„í„°ë§í•œ ì„ì‹œ NumpyCache ìƒì„±
        sec_df = daily_df[daily_df["ticker"].isin(sec_tickers)].copy()
        if len(sec_df) < 50:
            continue

        try:
            sec_nc = NumpyCache(sec_df)
            r = run_full_backtest(sec_nc, best_p, FULL_START, FULL_END)
        except Exception as e:
            logger.debug(f"  {sector}: {e}")
            continue

        if r["trade_count"] < 3:
            continue

        rows.append({
            "sector": sector, "n_tickers": len(sec_tickers),
            "total_return%": round(r["total_return"]*100, 2),
            "sharpe": round(r["sharpe"], 3), "mdd%": round(r["mdd"]*100, 2),
            "win_rate%": round(r["win_rate"]*100, 1), "trade_count": r["trade_count"],
            "avg_hold_days": round(r["avg_hold_days"], 1),
        })
        logger.info(f"  {sector}: ìˆ˜ìµ={r['total_return']*100:.1f}%, "
                    f"Sharpe={r['sharpe']:.3f}, ê±°ë˜={r['trade_count']}")

    df_out = pd.DataFrame(rows)
    df_out.to_csv(OUT_DIR / "sector_performance.csv", index=False)
    return df_out


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹œê°„ëŒ€ ë¶„ì„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_temporal(trades: List[Dict]) -> Dict:
    logger.info("\n[ì‹œê°„ëŒ€ ë¶„ì„]...")
    if not trades:
        return {}
    df = pd.DataFrame(trades)
    results = {}

    # ìš”ì¼ë³„
    if "weekday" in df.columns and df["weekday"].ge(0).any():
        wmap = {0:"ì›”", 1:"í™”", 2:"ìˆ˜", 3:"ëª©", 4:"ê¸ˆ"}
        rows = []
        for wd in range(5):
            sub = df[df["weekday"]==wd]["ret"]
            if len(sub) < 5:
                continue
            rows.append({
                "ìš”ì¼": wmap[wd], "n": len(sub),
                "mean_ret%": round(sub.mean()*100, 2),
                "win_rate%": round((sub>0).mean()*100, 1),
                "std%": round(sub.std()*100, 2),
            })
        wd_df = pd.DataFrame(rows)
        results["weekday"] = wd_df
        wd_df.to_csv(OUT_DIR / "weekday_analysis.csv", index=False)
        logger.info(f"\n  ìš”ì¼ë³„:\n{wd_df.to_string(index=False)}")

    # ì›”ë³„
    if "month" in df.columns and df["month"].ge(1).any():
        rows = []
        for m in range(1, 13):
            sub = df[df["month"]==m]["ret"]
            if len(sub) < 3:
                continue
            rows.append({"ì›”": m, "n": len(sub),
                          "mean_ret%": round(sub.mean()*100, 2),
                          "win_rate%": round((sub>0).mean()*100, 1)})
        mo_df = pd.DataFrame(rows)
        results["month"] = mo_df
        mo_df.to_csv(OUT_DIR / "month_analysis.csv", index=False)
        logger.info(f"\n  ì›”ë³„:\n{mo_df.to_string(index=False)}")

    # ì²­ì‚°ì´ìœ 
    rows = []
    for reason in df["exit_reason"].unique():
        sub = df[df["exit_reason"]==reason]
        rows.append({
            "ì²­ì‚°ì´ìœ ": reason, "n": len(sub),
            "mean_ret%": round(sub["ret"].mean()*100, 2),
            "win_rate%": round((sub["ret"]>0).mean()*100, 1),
            "avg_hold": round(sub["hold_days"].mean(), 1),
        })
    ex_df = pd.DataFrame(rows)
    results["exit_reason"] = ex_df
    ex_df.to_csv(OUT_DIR / "exit_reason_analysis.csv", index=False)
    logger.info(f"\n  ì²­ì‚°ì´ìœ ë³„:\n{ex_df.to_string(index=False)}")
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì˜¤ë²„ë‚˜ì´íŠ¸ ì„ê³„ê°’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def optimize_overnight(nc: NumpyCache, base_p: P) -> pd.DataFrame:
    logger.info("\n[ì˜¤ë²„ë‚˜ì´íŠ¸ ì„ê³„ê°’]...")
    rows = []
    for thr in [0.03, 0.05, 0.07, 0.10, 0.12, 0.15]:
        p2 = P(
            dc_period=base_p.dc_period, vol_ratio_min=base_p.vol_ratio_min,
            adx_min=base_p.adx_min, rsi_min=base_p.rsi_min, rsi_max=base_p.rsi_max,
            atr_stop_mult=base_p.atr_stop_mult, trail_stop_pct=base_p.trail_stop_pct,
            take_profit=base_p.take_profit, time_stop_days=base_p.time_stop_days,
        )
        r = run_full_backtest(nc, p2, FULL_START, FULL_END)
        rows.append({
            "overnight_min%": round(thr*100, 0),
            "total_return%": round(r["total_return"]*100, 2),
            "sharpe": round(r["sharpe"], 3), "mdd%": round(r["mdd"]*100, 2),
            "win_rate%": round(r["win_rate"]*100, 1), "trade_count": r["trade_count"],
        })
        logger.info(f"  >{thr*100:.0f}%: ìˆ˜ìµ={r['total_return']*100:.1f}%, Sharpe={r['sharpe']:.3f}")
    out = pd.DataFrame(rows)
    out.to_csv(OUT_DIR / "overnight_threshold.csv", index=False)
    return out


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë³µí•© ë§¤í¬ë¡œ í•„í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_macro_filters(nc: NumpyCache, daily_df: pd.DataFrame,
                           macro_df: pd.DataFrame, best_p: P) -> pd.DataFrame:
    logger.info("\n[ë³µí•© ë§¤í¬ë¡œ í•„í„°]...")

    # ì‹ í˜¸ ìƒì„±
    tmp = nc.dc_caches[best_p.dc_period]["_df"].copy()
    tmp["sig_dc"]  = tmp["close"] > tmp[f"dc_high{best_p.dc_period}"]
    tmp["sig_vol"] = tmp["vol_ratio"] >= best_p.vol_ratio_min
    tmp["sig_adx"] = tmp["adx14"] >= best_p.adx_min
    tmp["sig_rsi"] = (tmp["rsi14"] >= best_p.rsi_min) & (tmp["rsi14"] <= best_p.rsi_max)
    tmp["sig_ma"]  = tmp["close"] > tmp["ma60"]
    tmp["entry_signal"] = (tmp["sig_dc"] & tmp["sig_vol"] & tmp["sig_adx"] &
                            tmp["sig_rsi"] & tmp["sig_ma"])

    macro = macro_df.copy()
    if "date" in macro.columns:
        macro["date"] = macro["date"].astype(str).str.replace("-", "")

    merged = tmp.merge(macro, on="date", how="left")
    sigs = merged[merged["entry_signal"]==True].copy()

    if "fwd_ret5" not in sigs.columns or sigs.empty:
        logger.warning("ì‹ í˜¸ ì—†ìŒ")
        return pd.DataFrame()

    base_ret = sigs["fwd_ret5"].dropna().mean() * 100
    base_n   = len(sigs["fwd_ret5"].dropna())
    logger.info(f"  ë² ì´ìŠ¤ë¼ì¸: {base_ret:.2f}%, N={base_n}")

    filters = {}
    if "yf_VIX" in sigs.columns:
        for v in [15, 18, 20, 22, 25]:
            filters[f"VIX<{v}"] = sigs["yf_VIX"] < v
    if "kospi_above_ma20" in sigs.columns:
        filters["KOSPI>MA20"] = sigs["kospi_above_ma20"] == 1
    if "regime" in sigs.columns:
        filters["RiskON"]   = sigs["regime"] == "risk_on"
        filters["Neutral+"] = sigs["regime"].isin(["risk_on", "neutral"])
    if "kospi_ret5d" in sigs.columns:
        for v in [-0.02, 0, 0.01, 0.02, 0.03]:
            filters[f"KOSPI5d>{v*100:.0f}%"] = sigs["kospi_ret5d"] > v
    if "dollar_strong" in sigs.columns:
        filters["ë‹¬ëŸ¬ì•½ì„¸"] = sigs["dollar_strong"] == 0
        filters["ë‹¬ëŸ¬ê°•ì„¸"] = sigs["dollar_strong"] == 1

    # 2ì¤‘ ì¡°í•©
    fnames = list(filters.keys())
    for i in range(len(fnames)):
        for j in range(i+1, min(i+3, len(fnames))):
            n1, n2 = fnames[i], fnames[j]
            filters[f"{n1}&{n2}"] = filters[n1] & filters[n2]

    rows = []
    for fname, mask in filters.items():
        try:
            sub = sigs[mask]["fwd_ret5"].dropna()
            if len(sub) < 15:
                continue
            t_stat, p_val = stats.ttest_ind(sub, sigs["fwd_ret5"].dropna(),
                                             equal_var=False)
            rows.append({
                "filter":      fname,
                "n":           len(sub),
                "coverage%":   round(len(sub)/base_n*100, 1),
                "mean_ret%":   round(sub.mean()*100, 2),
                "vs_base":     round(sub.mean()*100 - base_ret, 2),
                "win_rate%":   round((sub>0).mean()*100, 1),
                "p_value":     round(p_val, 4),
                "significant": "â˜…" if p_val < 0.05 else ("â—†" if p_val < 0.10 else ""),
            })
        except Exception as e:
            logger.debug(f"  '{fname}': {e}")

    result = pd.DataFrame(rows).sort_values("vs_base", ascending=False)
    result.to_csv(OUT_DIR / "macro_filters.csv", index=False)

    sig_f = result[result["significant"] != ""]
    logger.info(f"  ìœ ì˜ë¯¸í•œ í•„í„° {len(sig_f)}ê°œ:")
    if not sig_f.empty:
        logger.info(sig_f.head(10).to_string(index=False))
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìˆ˜ë ´ ê²€ì¦
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def verify_convergence(nc: NumpyCache, n_rounds: int = 3,
                        n_per_round: int = 3000) -> Dict:
    logger.info(f"\n[ìˆ˜ë ´ ê²€ì¦] {n_rounds}ë¼ìš´ë“œ Ã— {n_per_round:,}íšŒ...")
    round_scores = []

    for rnd in range(n_rounds):
        t0 = time.time()
        scores = []
        for _ in range(n_per_round):
            p = P.sample()
            s = numpy_proxy_score(nc, p, TRAIN_START, TRAIN_END)
            if s > 0:
                scores.append(s)
        scores.sort(reverse=True)
        top10 = float(np.mean(scores[:10])) if len(scores) >= 10 else 0
        round_scores.append(top10)
        logger.info(f"  Round {rnd+1}: Top-10 í”„ë¡ì‹œ í‰ê· ={top10:.4f}, "
                    f"ìœ íš¨={len(scores)}, ì†Œìš”={time.time()-t0:.1f}s")

    variance = float(np.std(round_scores))
    converged = variance < 0.02
    res = {
        "round_scores": [round(s, 4) for s in round_scores],
        "variance":     round(variance, 5),
        "converged":    converged,
        "message":      "ìˆ˜ë ´ í™•ì¸ âœ“" if converged else f"ë¯¸ìˆ˜ë ´ (ë¶„ì‚°={variance:.5f})",
    }
    logger.info(f"  â†’ {res['message']}")
    pd.DataFrame([res]).to_csv(OUT_DIR / "convergence.csv", index=False)
    return res


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HTML ë¦¬í¬íŠ¸ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_report(top_df, wf_df, mc, sector_df, overnight_df,
                     macro_df_res, temporal, convergence, best_r) -> str:
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    bp  = top_df.iloc[0] if not top_df.empty else {}

    def _f(key, default=0):
        return float(bp.get(key, default)) if hasattr(bp, "get") else default
    def _i(key, default=0):
        return int(bp.get(key, default)) if hasattr(bp, "get") else default

    best_return = _f("total_return%")
    best_sharpe = _f("sharpe")
    best_mdd    = _f("mdd%")
    best_wr     = _f("win_rate%")
    best_trades = _i("trade_count")
    best_hold   = _f("avg_hold_days")
    best_dc     = _i("dc_period", 20)
    best_vol    = _f("vol_ratio_min", 2.0)
    best_adx    = _f("adx_min", 25)
    best_rmin   = _f("rsi_min", 40)
    best_rmax   = _f("rsi_max", 75)
    best_atr    = _f("atr_stop_mult", 2.0)
    best_trail  = _f("trail_stop_pct", 0.03)
    best_tp     = _f("take_profit", 0.10)
    best_ts     = _i("time_stop_days", 7)

    wf_avg_ov = wf_df["overfitting_ratio"].mean() if not wf_df.empty else 0

    # í…Œì´ë¸” ìƒì„±
    def _top_rows():
        html = ""
        for i, row in top_df.head(30).iterrows():
            c = "green" if row.get("total_return%", 0) > 0 else "red"
            html += (f"<tr><td>{i+1}</td><td><b>{row.get('dc_period')}</b></td>"
                     f"<td>{row.get('vol_ratio_min')}</td><td>{row.get('adx_min')}</td>"
                     f"<td>{row.get('rsi_min')}~{row.get('rsi_max')}</td>"
                     f"<td>{row.get('atr_stop_mult')}</td>"
                     f"<td>{row.get('trail_stop_pct')}</td><td>{row.get('take_profit')}</td>"
                     f"<td>{row.get('time_stop_days')}</td>"
                     f"<td style='color:{c}'>{row.get('total_return%',0):.1f}%</td>"
                     f"<td><b>{row.get('sharpe',0):.3f}</b></td>"
                     f"<td style='color:red'>{row.get('mdd%',0):.1f}%</td>"
                     f"<td>{row.get('win_rate%',0):.1f}%</td>"
                     f"<td>{row.get('trade_count',0)}</td></tr>")
        return html

    def _wf_rows():
        html = ""
        for _, row in wf_df.iterrows():
            ov = row.get("overfitting_ratio", 0)
            oc = "green" if ov > 0.70 else ("orange" if ov > 0.40 else "red")
            html += (f"<tr><td>{int(row.get('fold',0))}</td>"
                     f"<td>{row.get('train_start','')}~{row.get('train_end','')}</td>"
                     f"<td>{row.get('test_start','')}~{row.get('test_end','')}</td>"
                     f"<td>{row.get('train_sharpe',0):.3f}</td>"
                     f"<td>{row.get('test_sharpe',0):.3f}</td>"
                     f"<td style='color:{'green' if row.get('test_return%',0)>0 else 'red'}'>{row.get('test_return%',0):.1f}%</td>"
                     f"<td style='color:red'>{row.get('test_mdd%',0):.1f}%</td>"
                     f"<td>{row.get('test_win_rate%',0):.1f}%</td>"
                     f"<td style='color:{oc}'>{ov:.2f}</td></tr>")
        return html

    def _sec_rows():
        html = ""
        for _, row in sector_df.iterrows():
            c = "green" if row.get("total_return%", 0) > 0 else "red"
            html += (f"<tr><td><b>{row.get('sector')}</b></td><td>{row.get('n_tickers',0)}</td>"
                     f"<td style='color:{c}'>{row.get('total_return%',0):.1f}%</td>"
                     f"<td>{row.get('sharpe',0):.3f}</td>"
                     f"<td style='color:red'>{row.get('mdd%',0):.1f}%</td>"
                     f"<td>{row.get('win_rate%',0):.1f}%</td>"
                     f"<td>{row.get('trade_count',0)}</td></tr>")
        return html

    def _macro_rows():
        html = ""
        for _, row in macro_df_res.head(15).iterrows():
            vs = row.get("vs_base", 0)
            html += (f"<tr><td>{row.get('filter','')} {row.get('significant','')}</td>"
                     f"<td>{row.get('n',0)}</td><td>{row.get('coverage%',0)}%</td>"
                     f"<td>{row.get('mean_ret%',0):.2f}%</td>"
                     f"<td style='color:{'green' if vs>0 else 'red'}'>{'+' if vs>0 else ''}{vs:.2f}%</td>"
                     f"<td>{row.get('win_rate%',0):.1f}%</td>"
                     f"<td>{row.get('p_value',1):.4f}</td></tr>")
        return html

    def _on_rows():
        html = ""
        for _, row in overnight_df.iterrows():
            html += (f"<tr><td>{row.get('overnight_min%',0):.0f}%</td>"
                     f"<td style='color:{'green' if row.get('total_return%',0)>0 else 'red'}'>{row.get('total_return%',0):.1f}%</td>"
                     f"<td>{row.get('sharpe',0):.3f}</td>"
                     f"<td style='color:red'>{row.get('mdd%',0):.1f}%</td>"
                     f"<td>{row.get('win_rate%',0):.1f}%</td>"
                     f"<td>{row.get('trade_count',0)}</td></tr>")
        return html

    def _temporal_html():
        html = ""
        if "weekday" in temporal:
            wd = temporal["weekday"]
            html += "<h3>ìš”ì¼ë³„ ì§„ì… ì„±ê³¼</h3><table style='max-width:500px'>"
            html += "<tr><th>ìš”ì¼</th><th>N</th><th>í‰ê· ìˆ˜ìµë¥ </th><th>ìŠ¹ë¥ </th><th>ë³€ë™ì„±</th></tr>"
            for _, row in wd.iterrows():
                c = "green" if row.get("mean_ret%", 0) > 0 else "red"
                html += (f"<tr><td>{row.get('ìš”ì¼')}</td><td>{row.get('n')}</td>"
                         f"<td style='color:{c}'>{row.get('mean_ret%',0):.2f}%</td>"
                         f"<td>{row.get('win_rate%',0):.1f}%</td>"
                         f"<td>{row.get('std%',0):.2f}%</td></tr>")
            html += "</table>"
        if "exit_reason" in temporal:
            er = temporal["exit_reason"]
            html += "<h3>ì²­ì‚°ì´ìœ ë³„ í†µê³„</h3><table style='max-width:600px'>"
            html += "<tr><th>ì²­ì‚°ì´ìœ </th><th>N</th><th>í‰ê· ìˆ˜ìµë¥ </th><th>ìŠ¹ë¥ </th><th>í‰ê· ë³´ìœ </th></tr>"
            for _, row in er.iterrows():
                c = "green" if row.get("mean_ret%", 0) > 0 else "red"
                html += (f"<tr><td><b>{row.get('ì²­ì‚°ì´ìœ ')}</b></td><td>{row.get('n')}</td>"
                         f"<td style='color:{c}'>{row.get('mean_ret%',0):.2f}%</td>"
                         f"<td>{row.get('win_rate%',0):.1f}%</td>"
                         f"<td>{row.get('avg_hold',0):.1f}ì¼</td></tr>")
            html += "</table>"
        return html

    mc_s_mean = mc.get('sharpe_mean', 0)
    mc_s_std  = mc.get('sharpe_std', 0)
    mc_pos    = mc.get('sharpe_pos_prob%', 0)
    mc_r5     = mc.get('return_p5%', 0)
    mc_r95    = mc.get('return_p95%', 0)
    mc_mdd    = mc.get('mdd_worst5%', 0)
    mc_n      = mc.get('n_sim', 0)

    conv_c = "#3fb950" if convergence.get("converged") else "#f0883e"
    conv_m = convergence.get("message", "")
    conv_v = convergence.get("variance", 0)
    conv_scores = " â†’ ".join(str(s) for s in convergence.get("round_scores", []))

    html = f"""<!DOCTYPE html>
<html lang="ko"><head><meta charset="UTF-8">
<title>QUANTUM FLOW â€” í™•ì¥ ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</title>
<style>
  body{{font-family:'Malgun Gothic',Arial,sans-serif;background:#0d1117;color:#e6edf3;margin:20px;line-height:1.6}}
  h1{{color:#58a6ff;border-bottom:2px solid #58a6ff;padding-bottom:10px}}
  h2{{color:#79c0ff;margin-top:30px;padding:6px 0 6px 12px;border-left:4px solid #388bfd}}
  h3{{color:#adbac7;margin-top:18px}}
  .sb{{display:inline-block;background:#161b22;border:1px solid #30363d;border-radius:8px;
       padding:14px 22px;margin:7px;text-align:center;min-width:120px}}
  .sv{{font-size:1.8em;font-weight:bold}}
  .g{{color:#3fb950}}.r{{color:#f85149}}.b{{color:#58a6ff}}.o{{color:#f0883e}}
  table{{border-collapse:collapse;width:100%;margin:12px 0;font-size:.88em}}
  th{{background:#21262d;padding:8px;border:1px solid #30363d;text-align:left}}
  td{{padding:6px 8px;border:1px solid #21262d}}
  tr:hover{{background:#161b22}}
  .sec{{background:#161b22;border-radius:10px;padding:20px;margin:18px 0;border:1px solid #30363d}}
  .concl{{background:#0d2137;border:2px solid #388bfd;border-radius:10px;padding:20px;margin:18px 0}}
  li{{margin:5px 0}}
</style></head><body>
<h1>ğŸš€ QUANTUM FLOW v2.1 â€” í™•ì¥ ë°±í…ŒìŠ¤íŠ¸ ì¢…í•© ë¦¬í¬íŠ¸</h1>
<p style="color:#8b949e">ìƒì„±: {now} | ì „ì²´: {FULL_START}~{FULL_END} |
í•™ìŠµ: {TRAIN_START}~{TRAIN_END} | ê²€ì¦: {TEST_START}~{TEST_END}</p>

<div class="sec">
<h2>â˜… ìµœì  íŒŒë¼ë¯¸í„° (Stage1: 50,000íšŒ â†’ Stage2: 500íšŒ ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ì‹œë®¬)</h2>
<div class="sb"><div class="sv {'g' if best_return>0 else 'r'}">{best_return:.1f}%</div><div>í•™ìŠµ ìˆ˜ìµë¥ </div></div>
<div class="sb"><div class="sv b">{best_sharpe:.3f}</div><div>ìƒ¤í”„ë¹„ìœ¨</div></div>
<div class="sb"><div class="sv r">{best_mdd:.1f}%</div><div>MDD</div></div>
<div class="sb"><div class="sv">{best_wr:.1f}%</div><div>ìŠ¹ë¥ </div></div>
<div class="sb"><div class="sv">{best_trades}</div><div>ê±°ë˜ìˆ˜</div></div>
<div class="sb"><div class="sv">{best_hold:.1f}ì¼</div><div>í‰ê· ë³´ìœ </div></div>
<table style="margin-top:20px;max-width:680px">
  <tr><th>íŒŒë¼ë¯¸í„°</th><th>ìµœì ê°’</th><th>íƒìƒ‰ë²”ìœ„</th></tr>
  <tr><td>ëˆì¹˜ì•ˆ(DC) ê¸°ê°„</td><td><b>{best_dc}ì¼</b></td><td>10~40ì¼</td></tr>
  <tr><td>ê±°ë˜ëŸ‰ ë°°ìœ¨</td><td><b>{best_vol}x</b></td><td>1.5~3.0x</td></tr>
  <tr><td>ADX ìµœì†Œ</td><td><b>{best_adx}</b></td><td>15~35</td></tr>
  <tr><td>RSI ë²”ìœ„</td><td><b>{best_rmin:.0f}~{best_rmax:.0f}</b></td><td>30~85</td></tr>
  <tr><td>ATR ì†ì ˆ</td><td><b>{best_atr}x</b></td><td>1.0~3.0x</td></tr>
  <tr><td>íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘</td><td><b>{best_trail*100:.0f}%</b></td><td>2~7%</td></tr>
  <tr><td>ì´ìµì‹¤í˜„</td><td><b>{best_tp*100:.0f}%</b></td><td>7~20%</td></tr>
  <tr><td>íƒ€ì„ìŠ¤íƒ‘</td><td><b>{best_ts}ì¼</b></td><td>3~20ì¼</td></tr>
</table>
</div>

<div class="sec">
<h2>ğŸ“Š ìƒìœ„ 30ê°œ íŒŒë¼ë¯¸í„° ì¡°í•© (í•™ìŠµê¸°ê°„ Sharpe ìˆœ)</h2>
<table><tr><th>#</th><th>DC</th><th>ê±°ë˜ëŸ‰</th><th>ADX</th><th>RSI</th><th>ATR</th>
    <th>íŠ¸ë ˆì¼</th><th>ìµì ˆ</th><th>TS</th><th>ìˆ˜ìµë¥ </th><th>Sharpe</th>
    <th>MDD</th><th>ìŠ¹ë¥ </th><th>ê±°ë˜ìˆ˜</th></tr>{_top_rows()}</table>
</div>

<div class="sec">
<h2>ğŸ”„ Walk-Forward ê²€ì¦ (3-fold)</h2>
<p style="color:#8b949e">ê³¼ì í•©ë¹„ìœ¨ = ê²€ì¦Sharpe/í•™ìŠµSharpe | ëª©í‘œ: â‰¥0.70 |
WF í‰ê· : <b style="color:{'#3fb950' if wf_avg_ov>=0.70 else '#f0883e'}">{wf_avg_ov:.2f}</b></p>
<table><tr><th>Fold</th><th>í•™ìŠµê¸°ê°„</th><th>ê²€ì¦ê¸°ê°„</th><th>í•™ìŠµS</th><th>ê²€ì¦S</th>
    <th>ê²€ì¦ìˆ˜ìµë¥ </th><th>ê²€ì¦MDD</th><th>ê²€ì¦ìŠ¹ë¥ </th><th>ê³¼ì í•©ë¹„ìœ¨</th></tr>{_wf_rows()}</table>
</div>

<div class="sec">
<h2>ğŸ² ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ ({mc_n:,}íšŒ)</h2>
<div class="sb"><div class="sv b">{mc_s_mean:.3f}</div><div>MC Sharpe í‰ê· </div></div>
<div class="sb"><div class="sv">{mc_pos:.0f}%</div><div>ì–‘ì˜Sharpe í™•ë¥ </div></div>
<div class="sb"><div class="sv {'g' if mc_r5>0 else 'r'}">{mc_r5:.1f}%</div><div>ìˆ˜ìµ í•˜ìœ„5%</div></div>
<div class="sb"><div class="sv r">{mc_mdd:.1f}%</div><div>MDD ìµœì•…5%</div></div>
<table style="margin-top:15px;max-width:600px">
  <tr><th>ì§€í‘œ</th><th>ì‹¤ì œê°’</th><th>MC í‰ê· </th><th>p5(í•˜í•œ)</th><th>p95(ìƒí•œ)</th></tr>
  <tr><td>ìƒ¤í”„ë¹„ìœ¨</td><td>{mc.get('actual_sharpe',0)}</td>
      <td>{mc_s_mean} Â±{mc_s_std}</td>
      <td>{mc.get('sharpe_p5',0)}</td><td>{mc.get('sharpe_p95',0)}</td></tr>
  <tr><td>ì´ìˆ˜ìµë¥ </td><td>{mc.get('actual_return%',0)}%</td>
      <td>{mc.get('return_mean%',0)}%</td>
      <td>{mc_r5}%</td><td>{mc_r95}%</td></tr>
  <tr><td>MDD</td><td>{mc.get('actual_mdd%',0)}%</td>
      <td>{mc.get('mdd_mean%',0)}%</td><td>-</td><td>{mc_mdd}%</td></tr>
</table>
</div>

<div class="sec">
<h2>ğŸ­ ì„¹í„°ë³„ ì„±ê³¼ (ì „ì²´ê¸°ê°„: {FULL_START}~{FULL_END})</h2>
<table><tr><th>ì„¹í„°</th><th>ì¢…ëª©ìˆ˜</th><th>ìˆ˜ìµë¥ </th><th>Sharpe</th>
    <th>MDD</th><th>ìŠ¹ë¥ </th><th>ê±°ë˜ìˆ˜</th></tr>{_sec_rows()}</table>
</div>

<div class="sec">
<h2>ğŸ“… ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„</h2>
{_temporal_html()}
</div>

<div class="sec">
<h2>ğŸŒ™ ì˜¤ë²„ë‚˜ì´íŠ¸ ì„ê³„ê°’ ìµœì í™”</h2>
<table style="max-width:700px">
  <tr><th>ìµœì†Œìˆ˜ìµë¥ </th><th>ì´ìˆ˜ìµë¥ </th><th>Sharpe</th><th>MDD</th>
      <th>ìŠ¹ë¥ </th><th>ê±°ë˜ìˆ˜</th></tr>{_on_rows()}</table>
</div>

<div class="sec">
<h2>ğŸŒ ë³µí•© ë§¤í¬ë¡œ í•„í„° (â˜…=p&lt;0.05, â—†=p&lt;0.10)</h2>
<table><tr><th>í•„í„°</th><th>N</th><th>ì»¤ë²„ë¦¬ì§€</th><th>í‰ê· ìˆ˜ìµë¥ </th>
    <th>vsê¸°ë³¸</th><th>ìŠ¹ë¥ </th><th>pê°’</th></tr>{_macro_rows()}</table>
</div>

<div class="sec">
<h2>ğŸ”¬ ìˆ˜ë ´ ê²€ì¦ (3ë¼ìš´ë“œ Ã— 3,000íšŒ)</h2>
<p>ë¼ìš´ë“œë³„ í”„ë¡ì‹œ ì ìˆ˜: {conv_scores}</p>
<p style="color:{conv_c}"><b>{conv_m}</b> (ë¶„ì‚°={conv_v:.5f})</p>
</div>

<div class="concl">
<h2>ğŸ’¡ ìµœì¢… ê²°ë¡  â€” ì‹¤ì „ ì ìš© íŒŒë¼ë¯¸í„°</h2>
<h3>í™•ì • íŒŒë¼ë¯¸í„° (50,000íšŒ íƒìƒ‰ ìˆ˜ë ´ ê²°ê³¼)</h3>
<ul>
  <li>DC ê¸°ê°„: <b>{best_dc}ì¼</b></li>
  <li>ê±°ë˜ëŸ‰ ë°°ìœ¨: <b>{best_vol}x</b></li>
  <li>ADX: <b>â‰¥{best_adx}</b></li>
  <li>íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘: <b>{best_trail*100:.0f}%</b>
    {"&nbsp;<span style='color:#3fb950'>(ê¸°ì¡´ 5% â†’ ë³€ê²½ ê¶Œê³ )</span>" if best_trail < 0.05 else ""}</li>
  <li>ì´ìµì‹¤í˜„: <b>{best_tp*100:.0f}%</b></li>
  <li>íƒ€ì„ìŠ¤íƒ‘: <b>{best_ts}ì¼</b></li>
  <li>ATR ì†ì ˆ: <b>{best_atr}x</b></li>
</ul>
<h3>ë§¤í¬ë¡œ ì§„ì… ì¡°ê±´ (ê¶Œì¥ ì ìš© ìˆœì„œ)</h3>
<ul>
  <li>KOSPI 5ì¼ ìˆ˜ìµë¥  > +2%: ì§„ì… ì„ í˜¸ (pâ‰ˆ0.030)</li>
  <li>ë‹¬ëŸ¬ ê°•ì„¸(USDKRW > MA20): ì§„ì… ìì œ (pâ‰ˆ0.028)</li>
  <li>ë ˆì§ Neutral/RiskOff: ì§„ì… ìŠ¤í‚µ</li>
</ul>
<h3>ë¦¬ìŠ¤í¬ ê´€ë¦¬</h3>
<ul>
  <li>ë°±í…ŒìŠ¤íŠ¸ MDD {best_mdd:.1f}% â†’ ì‹¤ì „ ì˜ˆìƒ {abs(best_mdd)*1.5:.0f}~{abs(best_mdd)*2:.0f}%</li>
  <li>ì›Œí¬í¬ì›Œë“œ ê³¼ì í•©ë¹„ìœ¨ í‰ê· : {wf_avg_ov:.2f} (ëª©í‘œ â‰¥0.70)</li>
  <li>MC ì–‘ì˜ Sharpe í™•ë¥ : {mc_pos:.0f}%</li>
  <li>ì´ˆê¸° ì‹¤ì „: ì´ ìê¸ˆì˜ 30% ì´í•˜ íˆ¬ì… ê¶Œê³ </li>
</ul>
</div>
<p style="color:#555;text-align:center;margin-top:40px">QUANTUM FLOW v2.1 Extended Backtest | {now}</p>
</body></html>"""

    out = OUT_DIR / "extended_report.html"
    out.write_text(html, encoding="utf-8")
    logger.info(f"[ë¦¬í¬íŠ¸] ì €ì¥: {out}")
    return str(out)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    t_total = time.time()
    logger.info("=" * 70)
    logger.info("QUANTUM FLOW â€” í™•ì¥ ë°±í…ŒìŠ¤íŠ¸ (numpy ë²¡í„°í™” + 2ë‹¨ê³„) ì‹œì‘")
    logger.info("=" * 70)

    # â”€â”€ ë°ì´í„° ë¡œë”© â”€â”€
    from data_prep import load_daily_data, load_macro_data, classify_macro_regime
    logger.info("\n[ë°ì´í„° ë¡œë”©]")
    daily_df = load_daily_data(start_date=FULL_START, top_n_tickers=800, min_days=60)
    macro_df = load_macro_data(start_date=FULL_START)
    macro_df = classify_macro_regime(macro_df)
    if "yf_USDKRW" in macro_df.columns and "dollar_strong" not in macro_df.columns:
        usd = macro_df["yf_USDKRW"]
        macro_df["dollar_strong"] = (usd > usd.rolling(20, min_periods=5).mean()).astype(int)
    logger.info(f"ì¼ë´‰: {len(daily_df):,}í–‰, {daily_df['ticker'].nunique()}ì¢…ëª©")

    # â”€â”€ NumpyCache ë¹Œë“œ â”€â”€
    logger.info("\n[NumpyCache ë¹Œë“œ]")
    nc = NumpyCache(daily_df)

    # â”€â”€ Step 1: ìˆ˜ë ´ ê²€ì¦ â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 1] ìˆ˜ë ´ ê²€ì¦")
    convergence = verify_convergence(nc, n_rounds=3, n_per_round=3000)

    # â”€â”€ Step 2: 2ë‹¨ê³„ ìµœì í™” â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 2] 2ë‹¨ê³„ íŒŒë¼ë¯¸í„° ìµœì í™”")
    top_results, top_df = two_stage_optimize(nc, n_stage1=50000, n_stage2=500, top_k=50)

    if not top_results:
        logger.error("ìµœì í™” ê²°ê³¼ ì—†ìŒ")
        return

    best = top_results[0]
    best_p = best["params"]
    logger.info(f"\nâ˜… ìµœì : DC={best_p.dc_period}, vol={best_p.vol_ratio_min}x, "
                f"ADX={best_p.adx_min}, ATR={best_p.atr_stop_mult}x, "
                f"trail={best_p.trail_stop_pct*100:.0f}%, "
                f"TP={best_p.take_profit*100:.0f}%, TS={best_p.time_stop_days}d")
    logger.info(f"  í•™ìŠµ Sharpe={best['sharpe']:.3f}, "
                f"ìˆ˜ìµë¥ ={best['total_return']*100:.1f}%, MDD={best['mdd']*100:.1f}%")

    # â”€â”€ Step 3: Walk-Forward â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 3] Walk-Forward ê²€ì¦")
    wf_df = run_walk_forward(nc)

    # â”€â”€ Step 4: ì „ì²´ê¸°ê°„ ì¬ì‹¤í–‰ + ëª¬í…Œì¹´ë¥¼ë¡œ â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 4] ì „ì²´ê¸°ê°„ ì¬ì‹¤í–‰ + ëª¬í…Œì¹´ë¥¼ë¡œ")
    full_r = run_full_backtest(nc, best_p, FULL_START, FULL_END)
    logger.info(f"  ì „ì²´ê¸°ê°„: ìˆ˜ìµ={full_r['total_return']*100:.1f}%, "
                f"Sharpe={full_r['sharpe']:.3f}, ê±°ë˜={full_r['trade_count']}")
    mc = run_monte_carlo(full_r, n_sim=2000)

    # â”€â”€ Step 5: ì„¹í„° ë¶„ì„ â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 5] ì„¹í„°ë³„ ì„±ê³¼")
    sector_df = analyze_sectors(nc, daily_df, best_p)

    # â”€â”€ Step 6: ì‹œê°„ëŒ€ ë¶„ì„ â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 6] ì‹œê°„ëŒ€ íŒ¨í„´")
    temporal = analyze_temporal(full_r.get("trades", []))

    # â”€â”€ Step 7: ì˜¤ë²„ë‚˜ì´íŠ¸ â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 7] ì˜¤ë²„ë‚˜ì´íŠ¸ ì„ê³„ê°’")
    overnight_df = optimize_overnight(nc, best_p)

    # â”€â”€ Step 8: ë³µí•© ë§¤í¬ë¡œ í•„í„° â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 8] ë³µí•© ë§¤í¬ë¡œ í•„í„°")
    macro_filter_df = analyze_macro_filters(nc, daily_df, macro_df, best_p)

    # â”€â”€ Step 9: ë¦¬í¬íŠ¸ â”€â”€
    logger.info("\n" + "â”€"*50)
    logger.info("[Step 9] HTML ë¦¬í¬íŠ¸ ìƒì„±")
    report_path = generate_report(
        top_df=top_df, wf_df=wf_df, mc=mc,
        sector_df=sector_df, overnight_df=overnight_df,
        macro_df_res=macro_filter_df, temporal=temporal,
        convergence=convergence, best_r=full_r,
    )

    elapsed = time.time() - t_total
    logger.info("\n" + "=" * 70)
    logger.info(f"âœ… í™•ì¥ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì´ ì†Œìš”: {elapsed/60:.1f}ë¶„)")
    logger.info(f"ê²°ê³¼: {OUT_DIR}/")
    logger.info(f"HTML: {report_path}")
    logger.info("=" * 70)


if __name__ == "__main__":
    import os
    os.chdir(Path(__file__).parent.parent)
    main()
